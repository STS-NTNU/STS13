\section{Sublexical feature representation measures}
\label{subrep-features}

The model includes a set of features based on induced representations of character n-grams. The text were preprocessed by removing punctuation and extra whitespace, replacing numbers with their single digit word (one, two etc.) and lowercasing all text. Character n-grams including whitespace were generated from this text. The representations used are based on Brown Clustering \cite{}, Latent Semantic Indexing (LSI) \cite{} and loglinear skip-gram models \cite{}. The representations were trained on subsets of Wikipedia consisting of the first 12 million words (or $10^8$ characters) referred to as {\it Wiki8} and 125 million words ($10^9$ characters) referred to as {\it Wiki9}. The representations were trained with various parameters;  n-gram size, cluster size and term frequency cutoffs for all models.

The feature representations are obtained by Brown clustering \cite{brown1992class}, log linear skip-grams \cite{mikolov2013efficient} and LSI topic clustering \cite{deerwester1990indexing}. Feature representations where created with varying cluster/topic/vector size, skip-gram size, character n-gram size and term frequency cutoff. For log linear skip-gram models our intuition is that a larger skip-gram context is needed than the five or ten wide skip-grams used to train word based representations due to the smaller term vocabulary and dependency between adjacent n-grams. The representations trained for these measures used skip-gram widths of 25 or 50 terms. Term frequency cutoffs are also set to limit model size, but also potentially serves as a regularization on the resulting measure.

The specific measures used in the submitted systems were found by by training the regression model on the STS 2012 shared task data and evaluating on the STS 2013 test data. We used a stepwise forword selection method by comparing mean (but unweighted) correlation on the four test categories in order to identify the subset of measures to include in the final models.

The Brown clusters were trained using Percy Liangs software implementation \cite{liang2005semi}, while the LSI topic vectors and log linear skip-gram representations were trained using the Gensim topic modelling framework \cite{gensim_lrec}. We used a cosine distance measure between document vectors consisting of the centroid of the term representation vectors. For Brown clusters we use the normalized term frequency vectors using the cluster ids instead of the terms thenselves. For LSI topic representations the topic mixture for each term is used as the term representation. For the log linear skip-gram model word representations are extracted from the model weight matrix.

lsi tfidf

%%% Local Variables:  
%%% mode: latex 
%%% TeX-master: "sts14-ntnu" 
%%% End: 