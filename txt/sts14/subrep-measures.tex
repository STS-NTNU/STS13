\section{Sublexical feature representation measures}
\label{subrep-features}

We have created a set of similarity measures based on induced representations of character n-grams. The measures are based on similarity between document vectors which are the centroid of the individual term vector representations, which are here character n-grams. The vector representation are induced in an unsupervised manner from large unannoted corpora using methods using word clustering, topic learning and word representation learning.

In order to learn the representation vectors the text was preprocessed by removing punctuation and extra whitespace, replacing numbers with their single digit word (one, two etc.) and lowercasing all text. Character n-grams including whitespace were then generated from this text. We then trained Brown Clusters \cite{brown1992class}, Latent Semantic Indexing (LSI) topics \cite{deerwester1990indexing} and loglinear skip-gram models \cite{mikolov2013efficient} on the resulting character n-grams. The representation vectors were trained on subsets of Wikipedia consisting of the first 12 million words (or $10^8$ characters) referred to as {\it Wiki8} and 125 million words ($10^9$ characters) referred to as {\it Wiki9}. Representation vectors were trained with varying certain parameters;  n-gram size, cluster size and term frequency cutoffs for all models.
For log linear skip-gram models our intuition is that a larger skip-gram context is needed than the five or ten wide skip-grams used to train word based representations due to the smaller term vocabulary and dependency between adjacent n-grams. The representations trained for these measures used skip-gram widths of 25 or 50 terms. Term frequency cutoffs are also set to limit model size, but also potentially serves as a regularization on the resulting measure.

The Brown clusters were trained using Percy Liangs software implementation \cite{liang2005semi}, while the LSI topic vectors and log linear skip-gram representations were trained using the Gensim topic modelling framework \cite{gensim_lrec}. In addition we used TF-IDF \footnote{Term-Frequency Inverse Document Frequency, a common term weighting scheme, see for examople \cite{manning2008introduction}.} when training LSI topic models. We used a cosine distance measure between document vectors consisting of the centroid of the term representation vectors. For Brown clusters we use the normalized term frequency vectors using the cluster ids instead of the terms thenselves. For LSI topic representations the TF-IDF weighted topic mixture for each term is used as the term representation. For the log linear skip-gram model word representations are extracted from the model weight matrix.

%%% Local Variables:  
%%% mode: latex 
%%% TeX-master: "sts14-ntnu" 
%%% End: 