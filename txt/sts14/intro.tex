\section{Introduction}
\label{intro}

\blfootnote{
     % final paper: en-us version (to licence, a license)
    
    \hspace{-0.65cm}  % space normally used by the marker
    This work is licenced under a Creative Commons
    Attribution 4.0 International License.
    Page numbers and proceedings footer are added by
    the organizers.
    License details:
    \url{http://creativecommons.org/licenses/by/4.0/}
}

The Semantic Textual Similarity (STS) shared task aims at providing a unified framework for evaluating textual semantic similarity, ranging from exact semantic equivalence to completely unrelated texts. This is represented by the prediction of a similarity score between two sentences, drawn from a particular category of text, which ranges from 0 (different topics) to 5 (exactly equivalent) through six grades of semantic similarity \cite{agirre-EtAl:2013:*SEM1}.
This paper describes the NTNU submission to the SemEval 2014 STS shared task (Task~10). 
The appraoch is based on the lexical and distributional features of the baseline TakeLab system 
from the 2012 shared task \cite{saric2012takelab} and improves on it in three ways,
by adding two new categories of features and by using a bagging regression model to predict similarity scores. 
In the shared task evaluation, these improvements gave a substantial boost to the baseline system performance.

% something about Parthas features
The two new feature categories added are based on soft cardinality and on character n-grams. 
A set of features, based only on surface text information,
were extracted using soft cardinality \cite{chavez_text_2010}, which
have been used successfully for the STS task in previous SemEval editions
\cite{jimenez_soft_2012,jimenez_softcardinality_core:_2013}. 
The NTNU systems utilise an ensemble of 18 such measures with different similarity functions,
as further described in Section~\ref{sec:softcard}.

The similarity measures based on character n-gram feature representations are then
described in Section~\ref{subrep-features}.
These replace character n-gram features with cluster frequencies or vector values 
based on the n-gram collocational structure learned in an unsupervised manner from text data. 
Three different techniques are used to induce such representations: 
Brown clustering \cite{brown1992class}, log linear skip-gram representations \cite{mikolov2013efficient}, 
and Latent Semantic Indexing (LSI) topic vectors \cite{deerwester1990indexing}. 
A variety of such feature representations were trained on subsets of Wikipedia and the best performing ones 
were used for the new  measures, which are based on cosine similarity between the document vectors 
derived from each sentence in a given pair.

The two new feature categories are combined with the baseline features
through support vector machine regression (Section~\ref{sec:regression})
to create the actual NTNU systems (Section~\ref{sec:systems}).
As shown in Section~\ref{sec:results}, these new measures give competitive performance 
in the final evaluation results of the shared task.

%%% Local Variables:  
%%% mode: latex 
%%% TeX-master: "sts14-ntnu" 
%%% End: 
